# Chapter 08 배포

- 단일 프로세스로 된 모놀리식 애플리케이션의 배포는 매우 간단한 과정.
- 하지만 상호 의존성과 풍부한 기술 선택지를 갖춘 마이크로서비스는 완전히 별개.

## 1. 논리적에서 물리적으로

- 마이크로서비스를 바라보는 논리적 관점은 현실의 인프라스트럭처에 마이크로서비스를 실제로 실행할 때 수반되는 많은 복잡성을 숨겨준다.

### 1-1. 다수 인스턴스

- 두 마이크로서비스의 배포 토폴로지를 생각해보면 하나가 다른 하나와 대화하는 것만큼 간단하지 않다.
    - 우선 각 서비스의 인스턴스가 1개 이상 있을 가능성이 높다.
    - 서비스의 다수 인스턴스를 사용하면 더 많은 부하를 처리할 수 있고 단일 인스턴스 고장을 감내할 수 있으므로 시스템의 견고성이 향상될 수 있다.
    - 따라서 잠재적으로는 하나 이상의 인스턴스가 하나 이상의 인스턴스와 통신하게 된다.
    - 인스턴스 간 통신을 처리하는 방법은 통신 메커니즘의 특성에 좌우되지만, HTTP 기반의 API 형식을 사용한다고 가정하면 로드 밸런서만으로도 다른 인스턴스에 대한 요청 라우팅을 처리하기에 충분하다.
- 견고성을 이유로 서비스의 여러 인스턴스를 둔다면, 그 인스턴스가 모두 동일한 기본 하드웨어에 있는지 확인하고 싶을 것이다.
    - 다른 여러 데이터 센터에 여러 인스턴스를 분산해야 할 수도 있다.
    - 이것은 적어도 주요 클라우드 공급자와 작업할 때는 절대적인 고려 사항이다.
    - 관리형 머신 등과 관련해서 AWS, 애저, 구글 클라우드 모두 단일 머신에 대한 SLA를 제공하지 않으며 단일 `가용 영역 avilability zone`(이러한 공급자의 데이터 센터와 거의 같은 개념)에 대한 SLA도 제공하지 않는다.
    - 실제로 이 말은 배포하는 모든 솔루션이 여러 가용 영역에 분산돼야 한다는 것을 의미한다.

### 1-2. 데이터베이스

- 마이크로서비스의 내부 상태 관리는 감춰져야 하므로 상태 관리를 위해 사용되는 데이터베이스도 모두 마이크로서비스 내부에 숨겨져 있는 것으로 간주된다.
- 여러 인스턴스가 있다고 가정.
    - 마이크로서비스 인스턴스마다 자신만의 데이터베이스가 있어야 할까? => 그렇지 않다.
    - 논리적으로 동일한 서비스의 서로 다른 인스턴스 간에는 어느 정도의 공유 상태가 필요하다.
    - 상태에 액세스하고 조작하는 로직은 논리적으로 하나의 마이크로서비스에 여전히 유지된다.

#### 데이터 배포 및 확장

- 하지만 하나의 데이터베이스를 모든 인스턴스가 사용하는 것은 하부 데이터베이스의 중복성이나 확장성 요구에 대한 관심을 무시하는 것이다.
    - 대체로 데이터베이스 배포는 다양한 이유로 여러 머신에서 호스팅되며, 일반적인 예는 `기본 primary` 노드와 읽기 전용으로 지정된 하나 이상의 노드(이러한 노드를 일반적으로 `읽기 복제본 read replica`이라고 한다.) 간에 읽기 및 쓰기에 대한 부하를 분할하는 것이다.
    - 여러 개의 읽기 복제본과 하나의 쓰기 데이터베이스
    - 모든 읽기 전용 트래픽은 읽기 복제본 노드 중 하나로 들어가며, 읽기 노드를 추가하면 읽기 트래픽을 더 확장할 수 있다.
    - 관계형 데이터베이스가 작동하는 방식 때문에 머신을 추가해 쓰기 트래픽을 확장하기는 더 어려우므로(일반적으로 `샤딩 모델 sharding model`이 필요해 복잡성이 추가된다.) 더 확장하려면, 흔히 읽기 전용 트래픽을 읽기 복제본에 옮겨서 쓰기 노드에 더 많은 용량을 확보하곤 한다.
- 온프레미스 vs 클라우드
    - `온프레미스 on-premise` 방식으로 실행하는 조직은 비용상의 이유로 여러 데이터베이스를 공유 데이터베이스 인프라에서 호스팅할 가능성이 훨씬 더 높다. 하드웨어를 프로비저닝하고 관리하는 것은 괴로운 일이므로(적어도 지금까지 가상화된 인프라스트럭처에서 데이터베이스를 실행하지 않는 경향이 있다.) 그다지 원치 않게 된다.
    - 반면 공용 클라우드 공급자를 기반으로 운영하는 팀은 마이크로서비스별로 전용 데이터베이스 인프라스트럭처를 프로비저닝할 가능성이 훨씬 높다. 이 인프라스트럭처를 프로비저닝하고 관리하는 비용은 훨씬 저렴하다. 또한 각 마이크로서비스 소유자는 공유 서비스에 의존하지 않고 더 많은 제어를 할 수 있다.

### 1-3. 환경

- 소프트웨어를 배포하면, 그 소프트웨어는 환경에서 실행된다.
    - 각 환경은 일반적으로 서로 다른 용도로 사용되며, 보유할 수 있는 정확한 환경 수는 소프트웨어를 개발하는 방법과 최종 사용자에게 소프트웨어를 배포하는 방법에 따라 크게 달라질 수 있다.
- 일반적으로는 소프트웨어가 여러 가지 `사전 운영 preproduction` 환경을 통해 이동하는 것으로 생각하며, 각 환경은 소프트웨어를 개발하고 운영 준비 상태를 테스트할 수 있도록 특정 목적을 수행한다.
- 마이크로서비스는 최종적으로 사용자가 새 소프트웨어를 사용할 운영 환경에 도달하기 전에 여러 환경을 거치게 된다.
    1. 개발자의 로컬 노트북
    2. CI 환경
    3. 사전 운영 환경
    4. 운영 환경
- 모든 환경이 운영 환경의 정확한 복사본이 된다면 가장 이상적이다. => 소프트웨어가 운영 환경에 도달했을 때 동작할 것이란 더 큰 확신을 주기 때문이다.
    - 하지만 실제로는 비용이 너무 늘어나 운영 환경 전체 복사본을 여러 개 실행할 만한 여력이 없는 경우가 많다.
    - 가능한 한 빨리 소프트웨어의 작동 여부를 파악하는 것이 중요하다.
- 개발자에게 더 가까운 환경은 빠른 피드백을 제공하도록 조정되고 '운영 환경과 같은' 환경이 아닐 수 있음을 의미한다.
    - 하지만 운영 환경에 가까워질수록 문제를 포착할 수 있게끔 최종 운영 환경과 점점 유사해지길 바랄 것이다.

<br/>

## 2. 마이크로서비스 배포의 원칙

- 배포하는 방법의 선택지가 너무 많으므로, 이 분야에서는 몇 가지 핵심 원칙을 정하는 것이 중요하다.

### 2-1. 격리 실행

- 마이크로서비스 인스턴스가 자체 컴퓨팅 자원을 가진 격리된 방식으로 실행하라. 근처에서 실행 중인 다른 마이크로서비스 인스턴스에 영향을 미치지 않도록 실행돼야 한다.
- 마이크로비스로 향하는 여정 초기에는 모든 마이크로서비스 인스턴스를 단일 머신(하나의 물리 머신이나 `가상 머신 virtual machine`)에 밀어 넣고 싶은 유혹을 받을 수 있다.
    - 순전히 호스트 관리 관점에서 보면 이 모델이 더 간단하다.
    - 한 팀이 인프라스트럭처를 관리하고 다른 팀이 소프트웨어를 관리하는 세상에서 인프라스트럭처 팀의 작업 부하는 종종 관리해야 하는 호스트 수에 좌우된다.
    - 더 많은 서비스를 하나의 호스트에 넣으면 서비스의 수가 증가해도 호스트 관리 워크로드는 증가하지 않는다.
- 하지만 이 모델에는 몇 가지 문제가 있다. 우선 모니터링이 더 어려워질 수 있다.
    - 예를 들어 CPU를 추적할 때는 한 서비스의 CPU를 다른 서비스와 별도로 추적해야 할까? 아니면 호스트 CPU 전체를 신경 써야 할까? 부작용도 피하기 어려울 것이다.
    - 한 서비스에 상당한 부하가 걸리면 시스템의 다른 부분에 사용할 수 있는 자원이 줄어들게 된다.
- 한 배포가 다른 배포에 영향을 미치지 않도록 보장하는 것은 추가적인 고민거리가 생겨나므로 서비스 배포도 다소 복잡해질 수 있다.
    - 예를 들어 각 마이크로서비스가 공유 호스트에 설치해야 하는 서로 다른(잠재적으로 모순되는) `의존성 dependency`을 필요로 한다면 어떻게 해야 할까?
- 이 모델은 또한 팀의 자율성을 저해할 수 있다.
    - 서로 다른 팀의 서비스가 동일한 호스트에 설치된다면, 누가 해당 서비스를 위해 호스트를 구성할까? 아마도 결국 중앙에 위치한 팀에서 처리하게 될 것이다.
    - 즉, 서비스를 배포하는 데 더 많은 조율이 필요해진다.
- 기본적으로 동일한 머신에 많은 마이크로서비스 인스턴스를 실행하면 전체적으로 마이크로서비스 핵심 원칙 중 하나인 독립적 배포 가능성이 크게 약화된다.
    - 따라서 마이크로서비스 인스턴스를 격리된 상태로 실행해야 한다.
    - 각 마이크로서비스 인스턴스는 자체적으로 격리된 실행 환경이 있으며, 각각에게 필요한 의존성을 설치할 수 있고 전용 자원도 가진다.
- 컨테이너화 기술이 합류하면서 격리된 실행 환경을 프로비저닝하기 위해 그 어느 때보다 많은 방법을 갖추게 됐다.
    - 컨테이너는 격리 정도는 더 약하지만, 비용 효율이 더 높고 프로비저닝도 훨씬 더 빠르다.
- AWS 람다나 Heroku와 같은 더 추상화된 플랫폼에 마이크로서비스를 배포하는 경우에도 이러한 격리가 제공된다.
    - 플랫폼의 자체 특성에 따라 마이크로서비스 인스턴스는 내부의 컨테이너나 전용 VM 내에서 실행된다고 예상할 수 있다.
- 일반적으로 컨테이너와 관련된 격리 기술은 마이크로서비스 워크로드를 위한 더욱 당연한 선택으로 여겨질 만큼 충분히 개선됐다.
    - 격리 측면에서 컨테이너와 VM은 대부분의 워크로드에서 '만족할 만한' 수준으로 그 차이가 줄어들었다.

### 2-2. 자동화 집중

- 마이크로서비스의 수가 증가함에 따라 자동화가 점점 더 중요해지고 있다. 고수준의 자동화를 가능하게 할 기술을 선택하는 데 집중하고 자동화를 문화의 핵심 부분으로 채택하라.
- 대부분 수동으로 운영 프로세스를 관리하고 있다면 더 많은 서비스로 인해 작업할 사람이 더 필요하다는 사실을 의미한다.
    - 따라서 자동화에 끊임없이 주력해야 한다. 자동화는 개발자의 생산성을 유지하는 방법이다.
    - 각 서비스나 서비스 그룹을 직접 프로비저닝할 기능을 제공하는 것은 그들의 삶을 편리하게 만드는 비결이다.
- 자동화를 가능하게 하는 기술을 고르는 것은 호스트를 관리하는 데 사용되는 도구에서 시작된다.
    - 가상 머신을 시작하거나 종료하는 코드를 작성할 수 있는가?
    - 작성한 소프트웨어를 자동으로 배포할 수 있는가?
    - 수작업 없이 데이터베이스 변경 사항을 배포할 수 있는가?
    - 마이크로서비스 아키텍처의 복잡성을 억제하려면 자동화 문화를 수용하는 것이 핵심이다.

### 2-3. 코드형 인프라스트럭처

- 자동화를 용이하게 하고 정보 공유를 촉진하는 인프라스트럭처 구성을 기술하라. 환경을 재구축할 수 도록 이 코드를 `소스 제어 source control`에 저장하라.
- `코드형 인프라스트럭처 infrastructure as code`는 `기계 가독형 코드 machine-readable code`를 사용해 인프라스트럭처를 구성하는 개념이다.
    - chef나 puppet 파일에 서비스 구성을 정의하거나 설정에 필요한 bash 스크립트를 작성할 수도 있다.
    - 하지만 어떤 도구를 사용하든 소스 코드를 사용해 시스템이 알려진 상태로 전환할 수 있다.
    - 논란이 있지만 코드형 인프라스트럭처의 개념은 자동화를 구현하는 하나의 방법으로 간주되기도 하며, 자동화를 수행하는 방법을 알려주는 것 자체로도 다룰 만한 가치가 있다.

### 2-4. 무중단 배포

- 독립적인 배포 가능성을 더욱 강화해 서비스 사용자(사람 또는 마이크로서비스)에게 다운 타임 없이 새 버전의 마이크로서비스를 배포할 수 있어야 한다.
- 무중단 배포 기능을 구현하면 마이크로서비스를 개발하고 배포하는 과정에서 큰 진보를 이룰 수 있다.
    - `무중단 zero-downtime` 배포를 하지 못한다면, 소프트웨어를 릴리스할 때 업스트림 소비자에게 잠재적 중단을 경고하도록 그들과 조율해야 할 수도 있다.
- 여기서는 릴리스를 수행할 때 업스트림 소비자가 전혀 알아채지 못하게 하는 것이 목표다.
    - 이를 가능하게 하는 것은 마이크로서비스 특성에 크게 좌우된다. 이미 마이크로서비스와 소비자 사이에 미들웨어가 지원하는 비동기식 통신을 사용하고 있다면 쉽게 구현할 수 있다.
- `롤링 업그레이드 rolling upgrade`와 같은 개념은 유용할 수 있으며 쿠버네티스와 같은 플랫폼을 사용하면 삶이 훨씬 편리해지는 영역 중 하나다.
    - 롤링 업그레이드를 사용하면 새 버전이 배포되기 전까지 마이크로서비스가 완전히 종료되지 않는다.
    - 대신 새 버전의 소프트웨어를 실행하는 새 인스턴스가 증가함에 따라 기존 마이크로서비스 인스턴스가 서서히 감소한다.
    - 하지만 무중단 배포에 도움이 될 용도만으로 쿠버네티스를 적용한다면 지나친 과잉 작업이 될 가능성이 높다.

### 2-5. 기대 상태 관리

- 마이크로서비스를 지정된 상태로 유지할 수 있는 플랫폼을 사용해 장애가 발생하거나 트래픽이 증가할 때 필요하다면 새로운 인스턴스를 시작한다.
- `기기 상태 관리 desired state management`는 애플리케이션을 위한 인프라스트럭처의 요구 사항을 수동으로 개입하지 않고도 유지 관리하는 기능이다.
    - 실행 중인 시스템이 원하는 기대 상태가 더 이상 유지되지 않는 방식으로 변경되면 기반 플랫폼은 시스템을 원하는 상태로 되돌리려고 필요한 단계를 수행한다.
- 기대 상태 관리의 장점은 플랫폼 스스로 원하는 상태를 유지하는 방법을 관리한다는 것이다.
    - 이는 개발과 운영을 담당하는 사람들이 정확히 일이 어떻게 진행되고 있는지 걱정할 필요가 없게 해준다.
    - 즉, 처음에 원하는 기대 상태를 올바르게 정의하는 데만 집중하면 된다.
    - 인스턴스가 죽거나 하부 하드웨어가 고장 나거나 데이터 센터가 중단되는 등의 문제가 발생하는 경우 사람이 개입하지 않고도 플랫폼이 문제를 처리할 수 있음을 의미한다.
    - 쿠버네티스는 이 개념을 수용한 도구.

#### 전제 조건

- 기대 상태를 관리하려면 플랫폼에서 마이크로서비스 인스턴스를 자동으로 시작하는 방법이 필요하다.
    - 따라서 마이크로서비스 인스턴스에 대한 완전히 자동화된 배포는 올바른 상태 관리를 위한 확실한 전제 조건이다.
    - 또한 인스턴스가 시작하는 데 드는 시간을 신중하게 생각해야 할 수도 있다.
    - 사용자 부하를 처리하기에 충분한 컴퓨팅 리소스가 있는지 확인하는 데 기대 상태 관리를 사용하고 있다면, 인스턴스가 죽을 때 교체 인스턴스를 사용해 가능한 한 신속하게 공백을 메우길 원할 것이다.

#### 깃옵스

- `워브웍스 Weaveworks`가 객척한 비교적 최근의 개념인 `깃옵스 GitOps`는 기대 상태 관리와 코드형 인프라스트럭처의 개념을 통합한다.
    - 깃옵스는 원래 쿠버네티스와 작업하는 상화에서 고안됐으며 다른 사람들이 이전에 사용했던 워크플로를 설명하는 것은 틀림없지만 관련된 도구에 중점을 둔다.
- 깃옵스를 사용하면 인프라에 대해 원하는 기대 상태를 코드로 정의하고 소스 제어에 저장할 수 있다.
    - 기대 상태가 변경되면 일부 도구는 업데이트된 기대 상태가 실행 중인 시스템에 반영되도록 한다.
    - 이와 같은 아이디어는 개발자에게 애플리케이션 작업을 위한 간소화된 워크플로를 제공하기 위한 것이다.
- `플럭스 Flux`와 같은 도구를 사용하면 이러한 개념을 훨씬 더 쉽게 수용할 수 있다.
    - 물론 도구를 사용하면 기존의 작업 방식을 쉽게 변경할 수 있지만, 도구 때문에 새로운 방식을 강요할 수는 없다.
    - 달리 말하면 플럭스(또는 다른 깃옵스 도구)가 있다고 해서 기대 상태 관리나 코드형 인프라스트럭처를 수용하는 것은 아니라는 뜻이다.
- 쿠버네티스를 사용하고 플럭스와 같은 도구와 이 도구가 추진하는 워크플로를 채택해 기대 상태 관리 및 코드형 인프라스트럭처와 같은 개념을 도입하는 속도를 높일 수 있다.

<br/>

## 3. 배포 방법

- 마이크로서비스 워크로드에 사용할 수 있는 방식과 도구에는 많은 선택지가 있다.
    - 마이크로서비슥가 격리된 방식으로 실행되고 다운타임이 없는 이상적 방식으로 배포되는 것이 이상적.
    - 또한 자동화 문화를 수용하고 인프라스트럭처 및 애플리케이션 구성을 코드로 정의하며 이상적으로 기대 상태도 관리할 수 있는 도구를 선택.

### 3-1. 물리 머신

- 드문 방법.
- 사용자와 하부 하드웨어 사이에 가상화 또는 컨테이너화 계층이 없음을 의미하며, 이 방식은 몇 가지 다른 이유에서 점점 덜 보편적이게 됐다.
- 우선 물리 하드웨어에 직접 배포하면 자산 전체 활용도가 낮아질 수 있다.
    - 한 물리 머신에 하나의 마이크로서비스 인스턴스가 실행 중이고 하드웨어에서 제공하는 CPU, 메모리, I/O를 절반만 사용한다면 나머지 자원은 낭비된다.
    - 이런 문제로 대부분의 컴퓨팅 인프라스트럭처는 가상화 됨.

### 3-2. 가상 머신

- 기존 물리 머신을 더 작은 가상 머신으로 분할할 수 있게 함으로써 데이터 센터를 변화시켰다.
- 근본적으로 가상화를 사용하면 기본 머신을 여러 개의 더 작은 `가상 virtual` 머신으로 분할할 수 있다.
    - 이 가상 머신은 내부에서 실행되는 소프트웨어에 보통의 서버처럼 동작한다.
    - 하부의 CPU, 메모리, I/O, 스토리지에 해당하는 기능 일부를 각 가상 머신에 할당할 수 있으며, 이렇게 함으로써 마이크로서비스 인스턴스를 위해 더욱 격리된 실행 환경을 하나의 물리 머신에 밀어 넣을 수 있다.
- 각 VM은 매우 훌륭한 격리 수준이 보장된다.
    - 하지만 하부 머신이 망가지면 여러 마이크로서비스 인스턴스가 손실되는 문제는 여전히 있다.

#### 가상화 비용

- 점점 더 많은 가상 머신을 같은 하부 하드웨어에 넣을수록 VM 자체에서 가용한 컴퓨팅 리소스 측면에서 오히려 자원이 늘었음에도 효용이 떨어지는 현상 (`수확 체감 diminishing returns`)이 나타날 것이다.
- VM 가상화(`타입 2 가상화 type 2 virtualization`) vs 컨테이너 기반 가상화
- Type 2 Virtualization
    - VM은 물리적 인프라스트럭처에는 호스트 운영체제가 있다. 이 운영체제에서는 두 가지 중요한 임무를 맡은 `하이퍼바이저 hypervisor`라는 것을 실행하는데 하이퍼바이저의 두 가지 임무는 다음과 같다.
    - (1) CPU와 메모리 같은 자원을 가상 호스트에 물리 호스트로 매핑한다. (2) 제어 계층 역할을 하며 가상 머신 자체를 조작할 수 있게 한다.
    - VM 내부에는 완전히 다른 호스트처럼 보이는 것이 있으며, 이 호스트들은 자체 커널로 자체 운영체제를 실행할 수 있다.
    - 호스트는 하이퍼바이저에 의해 거의 밀폐된 머신으로 간주돼 하부의 물리 호스트와 다른 가상 머신에서 격리된 상태로 유지된다.
    - 이 가상화의 문제는 하이퍼바이저가 작업을 수행하기 위해 자원을 별도로 확보해야 한다는 것이다. 하이퍼바이저가 관리하는 호스트가 많을수록 더 많은 자원이 필요하며, 특정 시점에서 이 오버헤드는 물리 인프라스트럭처를 추가 분할하는 데 제약이 된다.

#### 마이크로서비스에 적합한가?

- 가상 머신은 격리 측면에서 매우 훌륭하지만 비용이 든다.
- 그동안 많은 조직에서 대규모 마이크로서비스 시스템을 실행하는 데 가상 머신을 사용해 큰 효과를 거둬왔다.

### 3-3. 애플리케이션 컨테이너

- 컨테이너는 서버 측 소프트웨어 배포에서 지배적 개념이 됐으며, 많은 사람에게 마이크로서비스 아키텍처를 패키징하고 실행하기 위한 실질적인 선택지가 됐다.
- 도커에 의해 대중화되고 쿠버네티스와 같은 지원 컨테이너 오케스트레이션 플랫폼과 결합된 컨테이너 개념은 대규모 마이크로서비스 아키텍처를 실행하기 위해 찾는 선택지가 됐다.

#### 다른 방식의 격리

- 컨테이너는 유닉스 계열의 운영체제에서 처음 등장.
- 동일한 머신에서 실행되는 컨테이너는 동일한 하부 커널을 사용한다.
    - 프로세스를 직접 관리하는 대신 컨테이너를 전체 시스템 프로세스 트리의 하위 트리에 대한 추상화로 생각할 수 있으며 커널이 모든 힘든 작업을 수행한다.
    - 이러한 컨테이너에는 커널이 처리하는 물리적 자원이 할당될 수 있다.
- 컨테이너를 실행하는 호스트에 대한 스택 다이어그램
    1. 하이퍼바이저가 필요 없다.
    2. 컨테이너에 커널이 없어 보이지만, 사실 하부 머신의 커널을 사용한다.
- 컨테이너를 사용하면 하이퍼바이저가 필요하지 않아 자원을 절약할 수 있다.
    - 리눅스 컨테이너는 비대한 가상 머신보다 프로비저닝이 훨씬 바르다.
    - VM이 시작하는 데 몇 분이 걸리는 경우는 드물지 않지만, 리눅스 컨테이너를 사용하면 몇 초면 된다.
    - 리소스 할당 측면에서 컨테이너 자체를 더 세밀하게 제어할 수 있으므로 하부 하드웨어를 최대한 활용하도록 훨씬 쉽게 설정을 조정할 수 있다.

#### 완전하지는 않다

- 하지만 리눅스 컨테이너가 문제없지는 않다.
    - 외부 세계를 라우팅할 수 있는 방법이 필요하며, 이는 많은 하이퍼바이저가 일반 가상화에서 수행하는 작업이다.
    - LXC와 같은 초기 기술을 사용할 대는 이 작업을 직접 해야 했다.
- 명심해야 할 또 다른 점은 리소스 관점에서 이러한 컨테이너가 격리된 것으로 간주될 수 있다는 사실이다.
    - 제한된 CPU, 메모리 등을 각 컨테이너에 할당할 수 있지만 가상 머신 격리 수준과 반드시 동일하지는 않다.
    - 또는 이 문제를 별도의 물리 머신을 사용함으로서 해결할 수 있다.

#### 윈도 컨테이너

- 윈도 사용자는 컨테이너가 윈도 운영체제에서 받아들여지지 않았기 때문에 리눅스를 사용하는 동시대 사람들을 부러워했던 것처럼 보인다.
    - 하지만 컨테이너가 완전히 지원되는 개념으로 바뀌면서 상황이 달라졌다.
    - 윈도 크기는 너무 커서 이미지 크기뿐 아니라 이미지를 실행하는 데 필요한 리소스 측면에서도 컨테이너가 매우 무거워졌다.

#### 도커

- 도커 이전에는 컨테이너에 대한 '이미지' 개념이 없었다.
    - 이러한 개념은 컨테이너 작업을 위한 훨씬 더 좋은 도구들과 함께 컨테이너를 훨씬 더 쉽게 사용하는 데 도움을 주었다.
- 도커 이미를 추상화하면 마이크로서비스 구현 방법에 대한 세부 정보를 숨길 수 있어 유용하다.
    - 우리는 마이크로서비스용 빌드가 빌드 산출물로 도커 이미지를 생성하고 그 이미지를 도커 레지스트리에 저장하도록 했다.
    - 도커 이미지의 인스턴스를 시작하면 사용되는 기본 기술(Go, Python, Node.js)에 관계없이 해당 인스턴스를 관리하는 일반적인 도구 집합이 있다.

#### 마이크로서비스에 대한 적합성

- 매우 적합
- 격리성을 확보할 수 있었지만 감당할 수 있는 비용.
- 하부 기술을 숨겨 다양한 기술 스택을 혼합 가능.
- 하지만 기대 상태 관리와 같은 개념을 구현하려면 이를 처리하기 위해 쿠버네티스와 같은 것이 필요.

### 3-4. 애플리케이션 컨테이너

- `웹로직 Weblogic`, `톰캣 Tomcat` 과 같은 것으로 배포하는 데 익숙하다면, 여러 개의 개별 서비스나 애플리케이션이 단일 애플리케이션 컨테이너에 존재하는 모델을 잘 알고 있을 것이다.
    - 애플리케이션 컨테이너도 단일 호스트에 상주한다.
    - 이 개념은 서비스가 상주하는 애플리케이션 컨테이너가 여러 인스턴스를 함께 그룹화하는 클러스터링 지원, 모니터링 도구 등과 같은 관리 용이성 측면에서 이점을 제공
- 여러 단점
    1. 필연적으로 기술 선택을 제한. 기술 스택을 받아들여야 한다.
    2. 제공하는 기능의 가치에도 의문

### 3-5. Paas

- Platform as a Service
- 단일 호스트보다 더 고수준의 추상화에서 작업하게 된다.
    - 일부는 자바 WAR 파일이나 루비 gem과 같은 특정 기술의 산출물을 가져와서 자동으로 프로비저닝하고 실행하는 데 의존
    - 시스템 확장 및 축소를 투명하게 처리하려고 시도.

### 3-6. Faas

- Function as a Service
- 지난 몇 년 동안 쿠버네티스에 근접한 유일한 기술은 서비리스다.
    - 하부의 컴퓨터가 중요하지 않고 다양한 기술을 가진 호스트를 포괄하는 상위 용어.

> - `서버리스 serverless`는 더 이상 서버와 관련 없다는 의미가 아니다. 단순히 개발자가 더 이상 서버에 대해 그렇게 많이 생각할 필요가 없다는 것을 의미한다.
> - 컴퓨팅 자원은 물리적 용량이나 제한을 관리할 필요 없이 서비스로 사용된다. 서비스 공급자는 점점 더 서버, 데이터 저장소 및 기타 인프라스트럭처 자원을 관리하는 책임을 갖는다.
> - 개발자는 자신들의 오픈 소스 솔루션을 설정할 수 있지만, 이는 서버, 큐, 부하를 관리해야 한다는 것을 의미한다.
    > 켄 프롬, '소트프웨어와 앱의 미래는 왜 서버리스인가?'

- FaaS는 서버리스의 주요 부분이 됐으므로 많은 경우에 두 용어는 서로 맞바꿔 사용할 수 있다.
    - 이는 데이터베이스, 큐, 스토리지 솔루션 등과 같은 다른 서버리스 제품의 중요성을 간과하게 되므로 안타까운 일이다.
    - 그럼에도 불구하고 FaaS가 논쟁을 지배하고 있다는 것은 FaaS가 얼마나 큰 관심을 불러일으키고 있는지를 말해준다.
- AWS Lambda.
    - 어떤 코드(함수)를 배포한다.
    - 해당 코드는 코드를 트리거하는 무언가 발생할 때까지 휴면 상태다.
    - 트리거가 어떤 것인지 결정하는 일은 사용자의 몫이다.
    - 예를 들면 특정 위치에 도착하는 파일, 메시지 큐에 나타나는 항목, HTTP를 통해 들어오는 호출 등이 될 수 있다.
- 함수가 트리거되면 실행되고 완료되면 종료된다.
    - 하부 플랫폼은 요구에 따라 이러한 기능을 시작하고 종료하며 함수의 동시 실행을 처리하므로 적절한 경우 한 번에 여러 복사본을 실행할 수 있다.
    - 사용하는 만큼만 비용을 지불하면 된다.

#### 제한 사항

- FaaS 구현은 이면에서 일종의 컨테이너 기술을 사용하고 이 기술은 숨겨져 있다.
    - 컨테이너 생성 걱정 X, 일부 패키징될 형태의 코드를 제공하기만 하면 된다.
    - 하지만 이는 정확히 실행할 수 있는 것에 대한 제어가 부족하다는 것을 의미한다. 결과적으로 개발자가 선택한 언어를 지원하려면 FaaS 제공자가 필요하다.

#### 문제점

1. `시작 시간 spin-up time` 개념을 해결하는 것이 중요하다.
- 개념적으로 함수는 필요하지 않으면 전혀 실행되지 않는다.
- 일부 런타임의 경우 새로운 런타임을 가동하는 데 오랜 시간이 걸리며, 이를 `콜드 스타트 cold start` 시간이라고 한다.
- 적어도 AWS는 런타임들이 `예열 warm` 상태로 유지되므로 인입된 요청은 이미 시작돼 실행 중인 인스턴스에서 처리된다.
2. 함수의 동적 확장 측면에서 실제로 문제가 될 수 있다.
- 함수의 최대 동시 호출 수에 대한 엄격한 제한. 주의 깊게 기록해야 할 수 있다.

#### 마이크로서비스와 매핑

- **마이크로서비스당 함수 매핑**
    - 단일 마이크로서비스 인스턴스는 단일 함수로 배포될 수 있다.
    - 비용 서비스를 REST 기반 마이크로서비스로 구현했다면 다양한 자원이 노출됐을 것.
    - 이 모델을 사용하면 이러한 자원에 대한 요청이 동일한 진입점을 통해 들어오므로, 인바운드 요청 경로를 기반으로 인바운드 후출을 적절한 기능 부분으로 돌려야 한다.
- **애그리거트당 함수 매핑**
    - 인스턴스를 더 작은 기능으로 어떻게 나눌 수 있을까?
    - 도메인 기반 설계를 사용하고 있다면 이미 애그리거트를 명시적으로 모델링했을 것이다.
    - 모든 로직이 함수 내에 완전히 포함돼 애그리거트의 수명주기 관리를 일관되게 구현하는 것이 더 수월해진다.
    - 이 모델을 사용하면 마이크로서비스 인스턴스가 더 이상 단일한 배포 단위로 매핑되지 않는다.
    - 이론상 서로 독립적으로 배포할 수 있는 여러 가지 함수로 구성된 논리적 개념에 가깝다.
- **더욱 세분화**
    - 더 작게 만들고 싶다면 애그리거트 당 기능을 더 작은 조각으로 나누고 싶은 유혹이 따른다.
    - 하지만 신중하게 결정 => 함수가 폭발적으로 증가할 수 있을 뿐 아니라 애그리거트의 힉샘 원칙 중 하나를 위한하기 때문
    - 애그리거트 자체의 일관성을 더 잘 관리하기 위해 애그리거트를 단일한 단위로 취급해야 한다.

#### 향후 전망

- 대부분의 개발자가 결국 사용하게 될 플랫폼 형태.
- FaaS 제품에 점점 더 많은 작업이 진행됨에 따라 주요 클라우드 공급자가 제공하는 FaaS 솔루션을 직접 사용할 수 없는 사람들도 점점 더 새로운 작업 방식을 활용하게 될 것.

<br/>

## 4. 어떤 배포가 적합할까?

> #### TIP
> - 더 진행하기 전에 현재 하고 있는 일이 효과가 있다면 계속하라.
> - 유행에 따라 기술적인 결정을 내리지 말자.

- 마이크로서비스 배포 원칙을 다시 검토하면서 집중했던 가장 중요한 측면 중 하나는 마이크로서비스의 격리를 보장하는 것이다.
    - 하지만 이를 기본 원칙으로 사용하면 각 마이크로서비스 인스턴스에 전용 물리 머신을 사용하도록 유도할 수 있다.
    - 당연히 이러한 방식은 비용이 많이 들 수 있고, 이미 논의한 것처럼 매우 강력한 몇몇 도구들을 이 방식에서는 사용하지 못할 수도 있다.
- 많은 장단점이 여기에 존재한다.
    - 사용 용이성, 격리, 친숙함 등과 비용의 균형을 맞추는 것은 감당하기 어렵다.
- '샘의 정말 기본적인 경험 규칙'

1. 고장 나지 않았다면 고치지 말라.
2. 당신이 만족한다고 느끼는 만큼 통제권을 포기한 다음 조금씩 더 포기하라. 모든 작업을 Heroku와 같은 훌륭한 PaaS에 맡길 수 있다면 그렇게 하고 만족하라. 정말로 여러분이 마지막 설정까지 일일이 손봐야 할까?
3. 마이크로서비스를 컨테이너화하는 일이 쉬운 일은 아니지만, 격리 비용에 대한 우수한 절충안임은 분명한다. 그리고 발생하는 작업에 대한 어느정도 제어권을 여전히 제공하면서 로컬 개발에 몇 가지 환상적인 이점을 가져온다. 향후에는 쿠버네티스를 기대하라.

- 많은 사람이 "쿠버네티스가 아니면 망한다!"라고 외치지만, 그것은 그다지 도움이 되지 않는다.
- 공용 클라이우에 있고 해당 문제가 배포 모델로 FaaS에 적합하다면 FaaS를 대신 사용하고 쿠버네티스는 고려하지 말라.
- 아마도 개발자의 생산성이 훨씬 높아질 것이다.

<br/>

## 5. 쿠버네티스와 컨테이너 오케스트레이션

- 여러 머신에서 컨테이너를 관리하는 솔루션.
- 도커는 이에 대해 두 번의 시도를 했다.
    - `Docker Swarm`
    - `Docker Swarm Mode`
- 이런 제품에 대한 여러 회사들의 노력에도 불구하고 쿠버네티스는 지난 몇 년 동안 이 분야를 지배.

### 5-1. 컨테이너 오케스트레이션에 대한 사례

- 쿠버네티스는 `Container Ochestration` 또는 `Container Scheduler` 등 다양하게 설명된다.
    - 이러한 플랫폼은 무엇이며 왜 필요할까?
- 컨테이너는 하부 머신에 자원들을 격리시키며 생성된다.
    - 도커와 같은 도구를 사용하면 컨테이너의 형태를 정의하고 머신에 컨테이너의 인스턴스를 생성할 수 있다.
    - 하지만 대부분의 솔루션은 시스템이 충분한 부하를 처리하거나 단일 노드의 고장을 감내하기 위한 `중복성 redundancy`을 갖도록 소프트웨어가 여러 머신에 정의되길 요구한다.
    - 컨테이너 오케스트레이션 플랫폼은 컨테이너 워크로드가 실행되는 방법과 위치를 다룬다. 이 맥락에서 `스케줄링 Scheduling`이라는 용어가 적합하다.
    - 운영자가 "이 작업이 실행되길 원합니다."라고 하면 오케스트레이터는 그 작업을 스케줄링하는 방법을 수행한다. 가용한 자원을 찾고, 필요한 경우 재할당하고 운영자를 위해 세세한 일을 처리한다.
- 또한 다양한 컨테이너 오케스트레이션 플랫폼은 기대 상태를 관리해 컨테이너들이 예상된 상태가 유지되도록 한다.
    - 워크로드를 분산하려는 방법을 지정할 수 있어 리소스 사용률, 프로세스 간 지연 시간, 또는 견고함의 원인을 최적화할 수 있다.

### 5-2. 쿠버네티스의 개념 엿보기

- 기본적으로 쿠버네티스 클러스터는 두 부분으로 구성된다.
1. 워크로드가 실행될 머신 집합. `노드 Node`
2. 이 `노드 Node`를 관리하는 일련의 제어 소프트웨어. `컨트롤 플레인 Control Plane`

- 노드는 컨테이너를 스케줄링하는 대신 `파드 Pod`라는 것을 스케줄링한다.
    - 파드는 함께 배포될 하나 이상의 컨테이너로 구성된다.
- `서비스 Service`
    - 안정적인 라우팅 엔드포인트로 간주할 수 있으며, 기본적으로 실행 중인 파드로부터 클러스터 내에서 사용할 수 있는 안정적인 네트워크 인터페이스로 매핑하는 방법.
    - 파드는 여러 가지 이유로 종료될 수 있으므로 일시적인 것으로 간주되지만 전체적인 `서비스`는 유지된다.
    - 쿠버네티스에서는 서비스를 배포하지 않고 서비스에 매핑되는 파드를 배포한다.
- `레플리카셋 Replica Set`
    - 파드들의 기대 상태를 정의
- `디플로이먼트 Deployment`
    - 파드와 레플리카셋에 대한 변경 사항을 적용하는 방법.
    - 롤링 업그레이드, 롤백, 노드 수 확장 등과 같은 작업을 수행할 수 있음.

### 5-3. 멀티테넌시와 페데레이션

- 효율성의 관점에서 하나의 쿠버네티스 클러스터에 가용한 모든 컴퓨팅 자원을 모아두고 조직 전체의 모든 워크로드를 그 클러스터에서 실행하길 원할 것이다.
    - 사용하지 않는 자원을 필요한 사람에게 자유롭게 재할당할 수 있으므로 하부 자원의 활용도가 높아지게 된다.
    - 이에 따라 비용도 적절히 줄어들 것.
- 문제는 쿠버네티스가 다양한 목적을 위해 서로 다른 마이크로서비스를 잘 관리할 수 있지만 플랫폼이 얼마나 `멀티테넌트 multitenanted`한지에 대해 제한이 있다.
    - 조직의 부서마다 다양한 자원에 대해 서로 다른 수준의 제어가 필요할 수 있다.
    - 이런 종류의 제어는 쿠버네티스에 내장되지 않았으며, 쿠버네티스의 범위를 다소 제한적으로 유지하려는 측면에서 현명한 결정으로 보인다.

1. 이와 같은 기능을 제공하는 '쿠버네티스 위에 구축된 플랫폼'을 채택
- 개발자는 쿠버네티스뿐 아니라 특정 벤더의 플랫폼을 사용하는 방법도 알아야 한다.
2. `페데레이션 모델 federated model`을 고려하는 것.
- 여러 개로 분리된 클러스터. 클러스트 위에 일부 소프트웨어 계층을 두어 필요하다면 모든 클러스터에 변경을 가할 수 있다.
- 리소스 풀링이 어려워진다는 단점이 있다.

### 5-4. 클라우드 네이티브 컴퓨팅 재단

- `CNCF`는 비영리 리눅스 재단에서 파생됐다.
- 쿠버네티스 자체뿐 아니라 쿠버네티스와 협업하거나 쿠버네티스 위에서 구축된 프로젝트도 지원.
- 마치 아파치 소프트웨어 재단의 역할을 떠올리게 한다.

### 5-5. 플랫폼과 이식성

- 쿠버네티스 자체는 플랫폼이 아니다. 기본적으로 제공되는 것은 컨테이너 워크로드를 실행할 수 있는 기능뿐이다.
    - 서비스 메시, 메시지 브로커, 로그 집계 도구 등과 같은 지원 소프트웨어를 스스로 설치해 자체 플랫폼을 구축해야 한다.
- 이것은 축복이자 저주가 될 수 있다.
    - 호환성이 매우 높은 도구 생태계. 허나, 너무 쉽게 많은 선택 사항에 압도당할 수 있다.
- 애플리케이션은 우리의 맞춤 플랫폼에 의존할 가능성이 굉장히 높기 때문에 한 쿠버네티스 클러스터에서 다른 클러스터로 이동시키려면 새 장소에서 해당 플랫폼을 다시 빌드해야할 수도 있다.
- 구축된 애플리케이션은 이론적으로 클러스터 간에 이식 가능하지만, 실제로 항상 가능하지는 않다.

### 5-6. Helm, Operator, CRD!

- 블랙 박스와 좀 더 유사한 방식으로 쿠버네티스 클러스터에서 실행.
- `Helm`은 스스로를 쿠버네티스의 '누락된 패키지 매니저'라고 칭하며 `Operator`는 초기 설치 관리할 수 있지만 애플리케이션의 지속적인 관리에 더 중점을 두는 것으로 보인다.
    - 서로의 대안으로 볼 수 있지만, 경우에 따라 두 가지를 함께 사용할 수도 있는 것은 우리를 혼란스럽게 만든다.
    - 초기 설치의 경우 헬름이 담당하고, 수명주기 작업의 경우 오퍼레이터가 담당.
- 이 분야에서 더 최근에 발전한 것이 있다면 `Custom Resource Definition, CRD`이다.
    - 핵심 쿠버네티스 API를 확장해 클러스터에 새로운 동작을 연결할 수 있다.
    - 기존 명령줄 인터페이스, 엑세스 제어 등과 매우 원활하게 통합되므로 사용자 정의 확장이 이질적으로 보이지 않는다는 점이다.
    - 구성 정보의 작은 비트 관리부터 `Istio`와 같은 서비스 메시 제어나 카프카와 같은 클러스터 기반 소프트웨어에 이르기까지 모든 작업에 CRD를 사용할 수 있다.

### 5-7. Knative

- 내부적으로 쿠버네티스를 사용해 개발자에게 FaaS 방식의 워크플로를 제공하는 것이 목표인 오픈 소스 프로젝트다.
    - 쿠버네티스는 그와 유사한 플랫폼들과 사용성을 비교할 때 그다지 개발자 친화적이지 않다.
    - Knative의 목표는 쿠버네티스의 복잡성을 숨기고 FaaS의 개발자 경험을 쿠버네티스에 가져오는 것이며, 결과적으로 개발 팀은 소프트웨어의 전체 수명주기를 더 쉽게 관리할 수 있음을 의미한다.
- 하지만 구글이 Knative에 대한 광범위한 업계 참여에 관심이 없다는 결정을 내린 것은 아쉽다.

### 5-8. 미래

- 쿠버네티스 광풍이 조만간 멈출 조짐은 없어 보임.

### 5-9. 사용해야 할까?

- 개발자가 쿠버네티스 설치를 통해 얻게 될 경험의 품질은 클러스터를 실행하는 팀의 효율성에 따라 좌우된다.
    - 이와 같은 이유로 온프레미스 쿠버네티스 경로를 따라간 다수의 대규모 조직은 이 작업을 전문 회사에 아웃소싱했다.
- 더 나은 방법은 관리형 클러스터를 사용하는 것이다.
    - 공용 클라우드를 사용할 수 있다면 구글, 애저, AWS에서 제공하는 완전 관리형 솔루션을 사용하라.
- "남들도 다 하고 있으니까"라고 말하면서 쿠버네티스를 해야 한다는 생각에 갇히지 마라.
    - 매우 위험한 선택이 될 수 있다. 따라서 자체 평가를 수행하라.
    - 하지만 솔직히 말해 개발자가 소수에 불과하고 마이크로서비스가 몇 개밖에 없는 경우 전적으로 관리형 플랫폼을 사용하더라도 쿠버네티스는 `오버엔지니어링 over-engineering`일 가능성이 높다.

<br/>

## 6. 점진적 제공 Progressive Delivery

- 성과가 좋은 회사는 성과가 낮은 회사보다 더 자주 배포하고 동시에 **변경 실패율도 훨씬 낮다.**

### 6-1. 배포와 릴리스의 분리

> 배포는 소프트웨어의 일부 버전을 특정 환경(운영 환경이 암시되는 경우가 많음)이 설치될 때 발생하는 것이다. 릴리스는 시스템이나 그 일부를 사용자가 사용할 수 있도록 만드는 것이다.

- 두 개념을 분리함으로써 사용자에게 실패 없이 운영 환경 설정에서 소프트웨어가 작동할 수 있다고 주장.

### 6-2. 점진적 제공으로

- 점진적 제공은 지속적 제공의 확장이자 새로 출시된 소프트웨어의 잠재적 영향을 제어할 수 있는 기능을 제공하는 기술.
- 새로운 기능이 고객에게 도달하는 방식에 대한 생각의 전환.
    - 더 이상 하나의 롤아웃이 아니라 단계적 활동이 될 것.

### 6-3. 기능 토글

- Feature Toggle
- 기능을 끄거나 켜는 데 사용할 수 있는 토글 뒤에 배포된 기능을 숨길 수 있다.
- 기능 토글을 관리하는 완전 관리형 솔루션이 있다고 함.

### 6-4. 카나리아 릴리스 Canary Release

> 실수는 사람이 할 수 있는 일이지만, 정말 일을 망치려면 컴퓨터가 필요하다.

- 우리는 모두 실수를 하며, 컴퓨터는 우리가 그 어느 때보다 더 빠르고 더 큰 규모로 실수를 할 수 있게 해준다.
    - 실수는 피할 수 없다는 점을 감안할 때 이러한 실수로 인한 영향을 제한하는 일을 하는 것이 합리적이다.
- 카나리아라는 새는 광산에 투입돼 마치 '조기 경보 시스템'처럼 광부들에게 위험한 가스의 존재를 경고
    - Canary Rollout은 제한된 일부 고객에게만 새로운 기능을 제공한다는 개념이다.
    - 롤아웃에 문제가 있는 경우 해당 부분의 고객만 영향을 받으며, 기능이 해당 카나리아 그룹에 대해 작동하는 경우 모든 사람이 새 버전을 볼 때까지 더 많은 고객에게 롤아웃할 수 있다.

### 6-5. 병렬 실행

- 동일한 기능의 서로 다른 구 현을 나란히 실행하고 기능에 대한 요청을 두 구현체로 보낸다.
    - 확실한 접근 방식은 동일한 서비스의 서로 다른 두 버전으로 보내고 결과를 비교하는 것이다.
    - 다른 대안은 동일한 서비스 내에서 해당 기능 구현을 모두 공존시키는 것인데, 이렇게 하면 비교하기 더 쉽다는 장점이 있다.

<br/>

# 참고 자료

- 마이크로서비스 아키텍처 구축, 샘 뉴먼 지음
